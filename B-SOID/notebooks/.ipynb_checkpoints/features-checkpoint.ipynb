{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "favorite-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"D:/IIT/DDP/DDP/B-SOID/\")\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from BSOID.bsoid import *\n",
    "from BSOID.preprocessing import *\n",
    "from BSOID.features.displacement_feats import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "crucial-benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered 21.78% of data\n",
      "filtered 1.79% of data\n",
      "filtered 0.48% of data\n",
      "filtered 0.62% of data\n",
      "filtered 0.74% of data\n"
     ]
    }
   ],
   "source": [
    "raw_dir = \"../../../data/videos/\"\n",
    "raw_files = [os.path.join(raw_dir, f) for f in os.listdir(raw_dir) if f.endswith(\".h5\")]\n",
    "\n",
    "bodyparts = []\n",
    "\n",
    "fdata = []\n",
    "for f in raw_files:\n",
    "    conf, pos = process_h5py_data(h5py.File(f, 'r'))\n",
    "    bsoid_data = bsoid_format(conf, pos)\n",
    "    data, perc_filt = likelihood_filter(bsoid_data, fps=30, bodyparts=np.arange(11))\n",
    "    print(f'filtered {round(perc_filt, 2)}% of data')\n",
    "    fdata.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-childhood",
   "metadata": {},
   "source": [
    "# Embedding for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "looking-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "@njit(fastmath=True)\n",
    "def KL(x, y):\n",
    "    n = x.shape[0]\n",
    "    x_sum = 0.0\n",
    "    y_sum = 0.0\n",
    "    kl = 0.0\n",
    "    z = 1e-11\n",
    "\n",
    "    for i in range(n):\n",
    "        x_sum += (x[i] + z)\n",
    "        y_sum += (y[i] + z)\n",
    "\n",
    "    for i in range(n):\n",
    "        kl += ((x[i] + z) / x_sum) * np.log(((x[i] + z) / x_sum) / ((y[i] + z) / y_sum))\n",
    "\n",
    "    return kl\n",
    "\n",
    "from scipy import signal\n",
    "def wavelet_transform(x):\n",
    "    fs, w0 = 30, 5\n",
    "    min_freq, max_freq, n_freq = 1, 15, 25\n",
    "    freqs = max_freq * (2 ** ((-np.arange(n_freq)/(n_freq - 1)) * np.log2(max_freq/min_freq)))\n",
    "    scales = w0*fs / (2*freqs*np.pi)\n",
    "    \n",
    "    Cs = (np.pi ** (-.25)) * np.exp(.25 * ((w0 - np.sqrt((w0**2)+2)) ** 2)) / np.sqrt(2 * scales)\n",
    "    wav = np.abs(signal.cwt(x, signal.morlet2, scales, dtype=np.complex128, w=w0))\n",
    "    wav = wav * Cs.reshape(-1,1)\n",
    "    return wav.T\n",
    "\n",
    "def wavelets(X):\n",
    "    return np.hstack([wavelet_transform(X[:,i]) for i in range(X.shape[1])])\n",
    "\n",
    "from itertools import combinations\n",
    "def comb_extractor(fdata, stride_window, temporal=False):\n",
    "    x_raw, y_raw = fdata['x'], fdata['y']\n",
    "    assert x_raw.shape == y_raw.shape\n",
    "    N, d = x_raw.shape\n",
    "\n",
    "    win_len = stride_window // 2\n",
    "    \n",
    "    x, y = np.zeros_like(x_raw), np.zeros_like(y_raw)\n",
    "    for i in range(x.shape[1]):\n",
    "        x[:,i] = smoothen_data(x_raw[:,i], win_len)\n",
    "        y[:,i] = smoothen_data(y_raw[:,i], win_len)\n",
    "\n",
    "    # displacement of points\n",
    "    dis = np.array([x[1:,:] - x[0:N-1,:], y[1:,:] - y[0:N-1,:]])\n",
    "    dis = np.linalg.norm(dis, axis=0)\n",
    "\n",
    "    # all combinations of links\n",
    "    links = []\n",
    "    for i, j in combinations(range(d), 2):\n",
    "        links.append(np.array([x[:,i] - x[:,j], y[:,i] - y[:,j]]).T)\n",
    "\n",
    "    wav = None\n",
    "    link_lens = np.vstack([np.linalg.norm(link, axis=1) for link in links]).T\n",
    "    if temporal:\n",
    "        wav = wavelets(link_lens)\n",
    "#         wav = wav / wav.sum(axis=1, keepdims=True)\n",
    "        wav = wav[stride_window:-1:stride_window]\n",
    "#     link_lens = link_lens[stride_window:-1:stride_window]\n",
    "    link_lens = windowed_feats(link_lens[1:], stride_window, mode=\"mean\")\n",
    "    \n",
    "    return link_lens, wav\n",
    "\n",
    "def extract_all_feats(fdata):\n",
    "    data = Parallel(n_jobs=6)(delayed(comb_extractor)(data, 10, True) for data in fdata)\n",
    "    feats, wav = [], []\n",
    "    for geom_feats, t_feats in data:\n",
    "        feats, wav = feats + [geom_feats], wav + [t_feats]\n",
    "    return np.vstack(feats), np.vstack(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "innocent-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats, wav = extract_all_feats(fdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-recipient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(dens_frac=0.0, dens_lambda=0.0, min_dist=0.0, n_epochs=500, n_neighbors=60,\n",
      "     verbose=True)\n",
      "Construct fuzzy simplicial set\n",
      "Sat May  8 02:29:01 2021 Finding Nearest Neighbors\n",
      "Sat May  8 02:29:01 2021 Building RP forest with 13 trees\n",
      "Sat May  8 02:29:01 2021 NN descent for 15 iterations\n",
      "\t 1  /  15\n",
      "\t 2  /  15\n",
      "\t 3  /  15\n",
      "\tStopping threshold met -- exiting after 3 iterations\n",
      "Sat May  8 02:29:07 2021 Finished Nearest Neighbor Search\n",
      "Sat May  8 02:29:07 2021 Construct embedding\n",
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n"
     ]
    }
   ],
   "source": [
    "from openTSNE import TSNEEmbedding\n",
    "from openTSNE.affinity import PerplexityBasedNN\n",
    "from openTSNE import initialization\n",
    "\n",
    "def plot(x, **kwargs):\n",
    "    plt.scatter(x[:,0], x[:,1], alpha=0.3, s=0.2)\n",
    "    plt.show()\n",
    "    \n",
    "mapper = umap.UMAP(n_components=2, n_neighbors=60, min_dist=0.0, n_epochs=500, verbose=True).fit(StandardScaler().fit_transform(feats))\n",
    "plot(mapper.embedding_)\n",
    "# affinities_train = PerplexityBasedNN(StandardScaler().fit_transform(feats), perplexity=100, metric=\"euclidean\", n_jobs=6, verbose=True)\n",
    "# init_train = initialization.pca(feats)\n",
    "# embedding0 = TSNEEmbedding(init_train, affinities_train, negative_gradient_method=\"fft\", n_jobs=6)\n",
    "# %time embedding1 = embedding0.optimize(n_iter=300, exaggeration=12, momentum=0.5, verbose=True, learning_rate=200)\n",
    "# plot(embedding1)\n",
    "\n",
    "# # %time embedding2 = embedding1.optimize(n_iter=2000, momentum=0.8, verbose=True, learning_rate=100)\n",
    "# # plot(embedding2)\n",
    "\n",
    "# affinities_train.set_perplexity(50)\n",
    "# embedding3 = embedding1.optimize(n_iter=300, exaggeration=12, momentum=0.8, learning_rate=100, verbose=True)\n",
    "# plot(embedding3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-selling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
