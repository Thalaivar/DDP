{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "micro-thousand",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruvlaad/anaconda3/envs/ddp/lib/python3.7/site-packages/umap/__init__.py:9: UserWarning: Tensorflow not installed; ParametricUMAP will be unavailable\n",
      "  warn(\"Tensorflow not installed; ParametricUMAP will be unavailable\")\n",
      "/Users/dhruvlaad/anaconda3/envs/ddp/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  \"update your install command.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import joblib\n",
    "import pysftp\n",
    "sys.path.insert(0, \"/Users/dhruvlaad/IIT/DDP/DDP/B-SOID/\")\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from BSOID.bsoid import *\n",
    "from BSOID.utils import *\n",
    "from BSOID.data import *\n",
    "from BSOID.preprocessing import *\n",
    "from BSOID.analysis import *\n",
    "from scipy import ndimage\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from itertools import combinations\n",
    "\n",
    "import scipy\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from eac import coassociation_matrix\n",
    "\n",
    "import networkx as nx\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from copy import deepcopy\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "greatest-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"/Users/dhruvlaad/IIT/DDP/data/paper/\"\n",
    "FIG1_SAVEDIR = os.path.join(SAVE_DIR, \"figure1\")\n",
    "FIG2_SAVEDIR = os.path.join(SAVE_DIR, \"figure2\")\n",
    "FIG3_SAVEDIR = os.path.join(SAVE_DIR, \"figure3\")\n",
    "DDP_SAVEDIR = os.path.join(SAVE_DIR, \"ddpplots\")\n",
    "for fdir in [FIG1_SAVEDIR, FIG2_SAVEDIR, FIG3_SAVEDIR, DDP_SAVEDIR]:\n",
    "    try: os.mkdir(fdir)\n",
    "    except FileExistsError: pass\n",
    "    \n",
    "LABEL_INFO_FILE = \"/Users/dhruvlaad/IIT/DDP/data/finals/label_info.sav\"\n",
    "MAX_LABEL = 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07d4aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_info():\n",
    "    with open(LABEL_INFO_FILE, \"rb\") as f:\n",
    "        label_info = joblib.load(f)\n",
    "    return label_info\n",
    "\n",
    "def get_transition_matrix(strain=None, exclude_strains=None, n=None, ns=None):\n",
    "    if strain is None:\n",
    "        strain_tmat = transition_matrix_across_strains(load_label_info(), MAX_LABEL)\n",
    "        if exclude_strains is not None:\n",
    "            for strain in exclude_strains:\n",
    "                del strain_usage[strain]\n",
    "    else:\n",
    "        strain_tmat = {strain: [{\"tmat\": transition_matrix(d[\"labels\"], MAX_LABEL)} for d in load_label_info()[strain]]}\n",
    "\n",
    "    tmats = []\n",
    "    for _, data in strain_tmat.items():\n",
    "        tmats.extend([d[\"tmat\"] for d in data])\n",
    "    \n",
    "    tmats = np.array(tmats)\n",
    "    if n is not None and ns is not None:\n",
    "        tmats = bootstrap_estimate(tmats, n, ns)[0]\n",
    "    \n",
    "    return tmats        \n",
    "        \n",
    "def get_usage(strain=None, exclude_strains=None, n=None, ns=None):\n",
    "    if strain is None:\n",
    "        strain_usage = proportion_usage_across_strains(load_label_info(), MAX_LABEL)\n",
    "        if exclude_strains is not None:\n",
    "            for strain in exclude_strains:\n",
    "                del strain_usage[strain]\n",
    "    else:\n",
    "        strain_usage = {strain: [{\"prop\": proportion_usage(d[\"labels\"], MAX_LABEL)} for d in load_label_info()[strain] ]}\n",
    "\n",
    "    usage = []\n",
    "    for _, data in strain_usage.items():\n",
    "        usage.extend([d[\"prop\"] for d in data])\n",
    "    \n",
    "    usage = np.vstack(usage)\n",
    "    if n is not None and ns is not None:\n",
    "        usage = bootstrap_estimate(usage, n, ns)\n",
    "    \n",
    "    return usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-cutting",
   "metadata": {},
   "source": [
    "# Figure 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-regression",
   "metadata": {},
   "source": [
    "## Keypoints on Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_filter_no_hour_trim(data, fps, conf_threshold, bodyparts):\n",
    "    N = data.shape[0]\n",
    "\n",
    "    # retrieve confidence, x and y data from csv data\n",
    "    conf, x, y = [], [], []\n",
    "    for col in data.columns:\n",
    "        if col.endswith('_lh'):\n",
    "            conf.append(data[col])\n",
    "        elif col.endswith('_x'):\n",
    "            x.append(data[col])\n",
    "        elif col.endswith('_y'):\n",
    "            y.append(data[col])\n",
    "    conf, x, y = np.array(conf).T, np.array(x).T, np.array(y).T\n",
    "    conf, x, y = conf[:,bodyparts], x[:,bodyparts], y[:,bodyparts]\n",
    "    \n",
    "    # take average of nose and ears\n",
    "    conf = np.hstack((conf[:,:3].mean(axis=1).reshape(-1,1), conf[:,3:]))\n",
    "    x = np.hstack((x[:,:3].mean(axis=1).reshape(-1,1), x[:,3:]))\n",
    "    y = np.hstack((y[:,:3].mean(axis=1).reshape(-1,1), y[:,3:]))\n",
    "\n",
    "    n_dpoints = conf.shape[1]\n",
    "    \n",
    "    filt_x, filt_y = np.zeros_like(x), np.zeros_like(y)\n",
    "\n",
    "    points_filtered_by_idx = np.zeros((n_dpoints,))\n",
    "    for i in range(n_dpoints):    \n",
    "        j, perc_filt = 0, 0\n",
    "\n",
    "        while j < N and conf[j,i] < conf_threshold:\n",
    "            perc_filt += 1\n",
    "            j += 1\n",
    "        \n",
    "        filt_x[0:j,i] = np.repeat(x[j, i], j)\n",
    "        filt_y[0:j,i] = np.repeat(x[j, i], j)\n",
    "        prev_best_idx = j\n",
    "\n",
    "        for j in range(j, N):\n",
    "            if conf[j,i] < conf_threshold:\n",
    "                filt_x[j,i] = x[prev_best_idx,i]\n",
    "                filt_y[j,i] = y[prev_best_idx,i]\n",
    "                perc_filt += 1\n",
    "            else:\n",
    "                filt_x[j,i], filt_y[j,i] = x[j,i], y[j,i]\n",
    "                prev_best_idx = j\n",
    "\n",
    "        points_filtered_by_idx[i] = perc_filt\n",
    "    \n",
    "    x, y, conf = filt_x, filt_y, conf\n",
    "    perc_filt = points_filtered_by_idx.max()\n",
    "    \n",
    "    return {'conf': conf, 'x': x, 'y': y}, perc_filt * 100 / N\n",
    "\n",
    "def download_video_and_keypt_data(input_csv, config_file, strain=None):\n",
    "    save_dir = os.path.join(FIG1_SAVEDIR, \"frame_keypoint_fig\")\n",
    "    try: os.mkdir(save_dir)\n",
    "    except FileExistsError: pass\n",
    "    df = pd.read_csv(input_csv)\n",
    "    if strain is not None:\n",
    "        df = df[df[\"Strain\"] == strain]\n",
    "    \n",
    "    done = False\n",
    "    session = ftplib.FTP(\"ftp.box.com\")\n",
    "    password = getpass(\"Box login password: \")\n",
    "    session.login(\"ae16b011@smail.iitm.ac.in\", password)\n",
    "    while not done:\n",
    "        try:\n",
    "            data = dict(df.iloc[np.random.randint(0, df.shape[0], 1)[0]])\n",
    "            data_fname, vid_fname = get_video_and_keypoint_data(session, data, save_dir)\n",
    "            done = True\n",
    "        except Exception as e:\n",
    "            session.quit()\n",
    "            session = ftplib.FTP(\"ftp.box.com\")\n",
    "            password = getpass(\"Box login password: \")\n",
    "            session.login(\"ae16b011@smail.iitm.ac.in\", password)\n",
    "    \n",
    "    print(f\"Extracting data from {os.path.join(save_dir, data_fname)}\")\n",
    "    conf, pos = process_h5py_data(h5py.File(os.path.join(save_dir, data_fname)))\n",
    "    bsoid_data = bsoid_format(conf, pos)\n",
    "    \n",
    "    with open(config_file, 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    fdata, perc_filt = likelihood_filter_no_hour_trim(bsoid_data, fps=30, conf_threshold=0.3, bodyparts=config[\"bodyparts\"])\n",
    "\n",
    "    shape = fdata['x'].shape\n",
    "    print(f'Preprocessed {shape} data, with {round(perc_filt, 2)}% data filtered')\n",
    "\n",
    "    with open(os.path.join(save_dir, \"rawdata.pkl\"), \"wb\") as f:\n",
    "        joblib.dump(fdata, f)\n",
    "\n",
    "def get_frames_from_video(n=10):\n",
    "    save_dir = os.path.join(FIG1_SAVEDIR, \"frame_keypoint_fig\")\n",
    "    try: os.mkdir(save_dir)\n",
    "    except FileExistsError: pass\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"rawdata.pkl\"), \"rb\") as f:\n",
    "        keypoints = joblib.load(f)\n",
    "    x, y = keypoints['x'], keypoints['y']\n",
    "\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    N = x.shape[0]\n",
    "\n",
    "    idxs = np.random.randint(0, N, n)\n",
    "    idxs.sort()\n",
    "    x, y = x[idxs], y[idxs]\n",
    "\n",
    "    count, frames = 0, []\n",
    "    \n",
    "    video_file = [os.path.join(save_dir, f) for f in os.listdir(save_dir) if f.endswith(\".avi\")][0]\n",
    "    video = cv2.VideoCapture(video_file)\n",
    "    success, image = video.read()\n",
    "    while success:\n",
    "        if count in idxs:\n",
    "            frames.append(image)\n",
    "        count += 1\n",
    "\n",
    "        success, image = video.read()\n",
    "    \n",
    "    assert count == N, f\"# of frames ({count}) in video does not match with # of keypoint-datapoints ({N})\"\n",
    "\n",
    "    for i, f in enumerate(frames):\n",
    "        cv2.imwrite(os.path.join(save_dir, f\"frame{i}.jpg\"), f)\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"keypointdata.pkl\"), \"wb\") as f:\n",
    "        joblib.dump([x, y], f)\n",
    "        \n",
    "def keypoint_plot(image_file, keypoint_data_file, idx, deg=0, xlim=None, ylim=None):\n",
    "    save_dir = os.path.join(FIG1_SAVEDIR, \"frame_keypoint_fig\")\n",
    "    with open(keypoint_data_file, \"rb\") as f:\n",
    "        x, y = joblib.load(f)\n",
    "    \n",
    "    x, y = x[idx], y[idx]\n",
    "\n",
    "    HEAD, BASE_NECK, CENTER_SPINE, HINDPAW1, HINDPAW2, BASE_TAIL, MID_TAIL, TIP_TAIL = np.arange(8)\n",
    "    link_connections = ([BASE_TAIL, CENTER_SPINE],\n",
    "                        [CENTER_SPINE, BASE_NECK],\n",
    "                        [BASE_NECK, HEAD],\n",
    "                        [BASE_TAIL, HINDPAW1], [BASE_TAIL, HINDPAW2],\n",
    "                        [BASE_TAIL, MID_TAIL],\n",
    "                        [MID_TAIL, TIP_TAIL])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    img = ndimage.rotate(plt.imread(image_file), deg)\n",
    "    ax.imshow(img, extent=[0, img.shape[0], 0, img.shape[1]])\n",
    "    \n",
    "    for link in link_connections:\n",
    "        h, t = link\n",
    "        plt.plot([x[h], x[t]], [y[h], y[t]], linewidth=2, color=\"y\")\n",
    "    \n",
    "    cmap = mpl.cm.get_cmap(\"tab20\")\n",
    "    for idx in np.arange(8):\n",
    "        plt.scatter(x[idx], y[idx], color=cmap(idx), s=30)\n",
    "    \n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    plt.savefig(os.path.join(save_dir, \"keypoint_plot.jpg\"), bbox_inches=\"tight\", pad_inches=0)\n",
    "    fig.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    img = ndimage.rotate(plt.imread(image_file), deg)\n",
    "    ax.imshow(img, extent=[0, img.shape[0], 0, img.shape[1]])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_axis_off()\n",
    "    plt.savefig(os.path.join(save_dir, \"original_image.jpg\"), bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be run only once\n",
    "input_csv = \"D:/IIT/DDP/data/paper/MergedMetaList_2019-04-18_strain-survey-mf-subset.csv\"\n",
    "download_video_and_keypt_data(input_csv, config_file=\"../config/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run till desirable frame is received\n",
    "get_frames_from_video(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-powell",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run once to generate keypoint plot\n",
    "idx = 3\n",
    "image_file = os.path.join(FIG1_SAVEDIR, \"frame_keypoint_fig\", f\"frame{idx}.jpg\")\n",
    "keypoint_file = os.path.join(FIG1_SAVEDIR, \"frame_keypoint_fig\", \"keypointdata.pkl\")\n",
    "keypoint_plot(image_file, keypoint_file, idx, deg=90, xlim=200, ylim=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-stone",
   "metadata": {},
   "source": [
    "# centroid velocity and corresponding behaviours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_for_plot(data_dir, label_info_file, config_file):\n",
    "    save_dir = os.path.join(FIG1_SAVEDIR, \"centroid_velocity_fig\")\n",
    "    try: os.mkdir(save_dir)\n",
    "    except FileExistsError: pass\n",
    "    \n",
    "    with open(label_info_file, \"rb\") as f:\n",
    "        label_info = joblib.load(f)\n",
    "    strain = random.sample(list(label_info.keys()), 1)[0]\n",
    "    data = random.sample(label_info[strain], 1)[0]\n",
    "    metadata, labels = data[\"metadata\"], data[\"labels\"]\n",
    "    \n",
    "    del label_info\n",
    "    print(\"getting raw data from JAX database...\")\n",
    "    \n",
    "    pose_dir, _ = get_pose_data_dir(data_dir, metadata[\"NetworkFilename\"])\n",
    "    _, _, movie_name = metadata['NetworkFilename'].split('/')\n",
    "    filename = f\"{pose_dir}/{movie_name[0:-4]}_pose_est_v2.h5\".replace('\\\\', '/')\n",
    "    password = getpass(\"JAX ssh login password: \")\n",
    "    with pysftp.Connection('login.sumner.jax.org', username='laadd', password=password) as sftp:\n",
    "        sftp.get(filename)\n",
    "    print(f\"Extracted: {filename}\")\n",
    "    \n",
    "    with open(config_file, 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        \n",
    "    filename = f\"{movie_name[0:-4]}_pose_est_v2.h5\"\n",
    "    conf, pos = process_h5py_data(h5py.File(filename, \"r\"))\n",
    "    data = bsoid_format(conf, pos)\n",
    "    fdata, perc_filt = likelihood_filter(data, config[\"fps\"], config[\"conf_threshold\"], config[\"bodyparts\"], end_trim=5, clip_window=-1)\n",
    "    if perc_filt > 10:\n",
    "        strain, mouse_id = metadata['Strain'], metadata['MouseID']\n",
    "        print(f'mouse:{strain}/{mouse_id}: % data filtered from raw data is too high ({perc_filt} %)')\n",
    "    \n",
    "    if labels.size < fdata['x'].shape[0]:\n",
    "        print(f\"# of labels ({labels.size}) not equal to # of datapoints ({fdata['x'].shape[0]})\")\n",
    "        fdata['x'], fdata['y'] = fdata['x'][:labels.size], fdata['y'][:labels.size]\n",
    "    \n",
    "    keypoints = {'x': fdata['x'], 'y': fdata['y'], \"labels\": labels}\n",
    "    with open(os.path.join(save_dir, \"metadata.pkl\"), \"wb\") as f:\n",
    "        joblib.dump(keypoints, f)\n",
    "    \n",
    "    os.remove(filename)\n",
    "    \n",
    "def plot_centroid_velocity(fps, tspan=None):\n",
    "    save_dir = os.path.join(FIG1_SAVEDIR, \"centroid_velocity_fig\")\n",
    "    try: os.mkdir(save_dir)\n",
    "    except FileExistsError: pass\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"metadata.pkl\"), \"rb\") as f:\n",
    "        metadata = joblib.load(f)\n",
    "    \n",
    "    from BSOID.preprocessing import smoothen_data\n",
    "    centroid_idx = 2\n",
    "    x, y = metadata['x'][:,centroid_idx], metadata['y'][:,centroid_idx]\n",
    "    x, y = smoothen_data(x, 3), smoothen_data(y, 3)\n",
    "    x, y = (x[1:] - x[0:-1]) * 0.0264 / (1 / fps), (y[1:] - y[0:-1]) * 0.0264 / (1 / fps)\n",
    "    x, y = x/100, y/100\n",
    "    vel = np.sqrt(x ** 2 + y ** 2)\n",
    "    \n",
    "    def plot_time_series(vel, fps, final=False):\n",
    "        t = np.arange(vel.shape[0])\n",
    "        fig, ax = plt.subplots(figsize=(12,2.5))\n",
    "        ax.plot(t, vel, color=\"k\", linewidth=1)\n",
    "        ax.set_ylim([-0.01*vel.max(), 1.1 * vel.max()])\n",
    "        ax.set_xlim([0, t.max() + 1])\n",
    "        if final:\n",
    "            ax.set_xticks([t.max() + 1])\n",
    "            ax.set_xticklabels([f\"{int(t.max() / fps)} seconds\"], fontsize=10)\n",
    "        ax.set_yticks([0, vel.max()])\n",
    "        ax.set_yticklabels([\"0\", f\"{round(vel.max(), 2)} m/s\"])\n",
    "        ax.yaxis.tick_right()\n",
    "        ax.tick_params(axis=u'both', which=u'both',length=0)\n",
    "        for _, spine in ax.spines.items():\n",
    "            spine.set_visible(False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, \"centroid_velocity.jpg\"), bbox_inches=\"tight\", pad_inches=0.5)\n",
    "        plt.show()\n",
    "      \n",
    "    if tspan is None:\n",
    "        plot_time_series(vel, fps)\n",
    "    else:\n",
    "        plot_time_series(vel[tspan[0]:tspan[1]], fps, final=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/projects/kumar-lab/StrainSurveyPoses\"\n",
    "label_info_file = '/Users/dhruvlaad/IIT/DDP/data/finals/label_info.sav'\n",
    "config_file = \"../config/config.yaml\"\n",
    "# get_metadata_for_plot(data_dir, label_info_file, config_file)\n",
    "\n",
    "tspan = [67000, 72000]\n",
    "plot_centroid_velocity(30, tspan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-chair",
   "metadata": {},
   "source": [
    "## ethrogram corresponding to centroid velocity plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ethogram_plot(tspan, fps, groups, group_names):\n",
    "    cmap = mpl.cm.get_cmap('tab20')\n",
    "\n",
    "    with open(os.path.join(FIG1_SAVEDIR, \"centroid_velocity_fig\", \"metadata.pkl\"), \"rb\") as f:\n",
    "        metadata = joblib.load(f)\n",
    "    \n",
    "    labels = metadata[\"labels\"][tspan[0]:tspan[1]]\n",
    "    height, N = 1, len(labels)\n",
    "    \n",
    "    labels = [groups[l] for l in labels]\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12, 3), gridspec_kw={'height_ratios': [1, 2.75]})\n",
    "    i, pat = 0, []\n",
    "    while i < len(labels):\n",
    "        j = i + 1\n",
    "        while j < len(labels) and labels[i] == labels[j]:\n",
    "            j += 1\n",
    "        pat.append(patches.Rectangle((i, 0), (j - i + 1), height, color=cmap(labels[i])))\n",
    "        i = j\n",
    "    \n",
    "    [ax[0].add_patch(p) for p in pat]\n",
    "    ax[0].set_yticks([])\n",
    "    ax[0].set_xticks([])\n",
    "    for _, spine in ax[0].spines.items():\n",
    "        spine.set_visible(False)\n",
    "    ax[0].set_xlim([0, N])\n",
    "    ax[0].set_ylabel(\"Ethogram\", fontsize=10)\n",
    "\n",
    "    height = 0.5\n",
    "    k, ylocs = 0, {}\n",
    "    for _, idx in group.items():\n",
    "        lab = group_names[idx]\n",
    "        if idx not in ylocs:\n",
    "            ylocs[idx] = (k*(height + 0.1), lab)\n",
    "            k += 1\n",
    "    \n",
    "    i, pat = 0, []\n",
    "    while i < len(labels):\n",
    "        j = i + 1\n",
    "        while j < len(labels) and labels[i] == labels[j]:\n",
    "            j += 1\n",
    "        y = ylocs[labels[i]][0]\n",
    "        pat.append(patches.Rectangle((i, y), (j - i + 1), height, color=cmap(labels[i])))\n",
    "        i = j    \n",
    "\n",
    "    [ax[1].add_patch(p) for p in pat]\n",
    "    ax[1].set_xlim([0, N])\n",
    "    ax[1].set_ylim([0 - 0.1, (k+1)*(height+0.1) - 0.1])\n",
    "\n",
    "    for _, lab in ylocs.items():\n",
    "        y = lab[0] + (height / 2)\n",
    "        ax[1].plot(np.arange(N), y * np.ones((N,)), color='gray', linestyle='-', linewidth=0.77, alpha=0.1)\n",
    "    ax[1].set_yticks([lab[0] + (height / 2) for _, lab in ylocs.items()])\n",
    "    ax[1].set_yticklabels([lab[1] for _, lab in ylocs.items()], fontsize=9)\n",
    "    ax[1].yaxis.tick_right()\n",
    "    ax[1].set_ylabel(\"Phenotypes\", fontsize=10)\n",
    "    ax[1].set_xticks([N])\n",
    "    ax[1].set_xticklabels([f\"{int((tspan[1] - tspan[0]) / fps)} seconds\"], fontsize=10)\n",
    "    ax[1].tick_params(axis=u'both', which=u'both',length=0)\n",
    "    for _, spine in ax[1].spines.items():\n",
    "        spine.set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"ethogram_fig\", \"ethogram_plot.jpg\"), bbox_inches=\"tight\", pad_inches=0, dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = None\n",
    "group_names = None\n",
    "\n",
    "ethogram_plot(tspan, 30, groups, group_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-tackle",
   "metadata": {},
   "source": [
    "# Figure 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-polish",
   "metadata": {},
   "source": [
    "## proportion usage plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "def behaviour_usage_plot(label_info_file, max_label, n=30, ns=250):\n",
    "    save_dir = os.path.join(FIG2_SAVEDIR, \"behaviour_usage_figure\")\n",
    "    try: os.mkdir(save_dir)\n",
    "    except FileExistsError: pass\n",
    "    \n",
    "    with open(label_info_file, \"rb\") as f:\n",
    "        label_info = joblib.load(f)\n",
    "    \n",
    "    strain_props = proportion_usage_across_strains(label_info, max_label)\n",
    "    props = []\n",
    "    for strain, data in strain_props.items():\n",
    "        for d in data:\n",
    "            props.append(d[\"prop\"])\n",
    "    props = np.vstack(props)\n",
    "    \n",
    "    prop_mean = props.mean(axis=0)\n",
    "    prop_stds = bootstrap_estimate(props, ns=ns, n=n)\n",
    "    idx = np.argsort(-prop_mean)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    ax.errorbar(\n",
    "        x=np.arange(prop_mean.size), \n",
    "        y=prop_mean[idx], \n",
    "        yerr=prop_stds[idx], \n",
    "        fmt=\"-k\", \n",
    "        ecolor=(0.59, 0.59, 0.61, 0.85), \n",
    "        elinewidth=4.5, \n",
    "        capsize=4,\n",
    "        linewidth=3\n",
    "    )\n",
    "    ax.set_xlabel('Sorted Phenotypes', fontsize=18)\n",
    "    ax.set_ylabel('Proportion Usage', fontsize=18)\n",
    "    ax.set_ylim([0, 1.1 * (prop_mean[idx[0]] + prop_stds[idx[0]])])\n",
    "    sns.despine(trim=True)\n",
    "    ax.tick_params(axis='x', labelrotation=0, labelsize=14)\n",
    "    plt.savefig(os.path.join(save_dir, \"behaviour_usage_plot.jpg\"), dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    fig.show()\n",
    "    \n",
    "    prop = {\"Strain\": []}\n",
    "    for i in range(max_label):\n",
    "        prop[f\"prop {i}\"] = []\n",
    "        \n",
    "    for strain, data in strain_props.items():\n",
    "        for d in data:\n",
    "            if \"BTBR\" in strain:\n",
    "                strain = \"$\\mathregular{BTBR T^+ Itpr3^{tf}/J}$\"\n",
    "            prop[\"Strain\"].append(strain)\n",
    "            [prop[f\"prop {i}\"].append(d[\"prop\"][i]) for i in range(max_label)]\n",
    "    prop = pd.DataFrame.from_dict(prop)\n",
    "    usage = prop.groupby(\"Strain\").median().T\n",
    "    ax = sns.heatmap(usage, xticklabels=True, yticklabels=True, cbar_kws={\"pad\": 0.01}, cmap=\"hot\")\n",
    "    plt.gcf().set_size_inches(15.5, 4)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel(\"Phenotypes\", fontsize=13)\n",
    "    ax.set_yticks(range(0, max_label, 10))\n",
    "    ax.tick_params(axis='x', labelsize=10)\n",
    "    ax.set_yticklabels(range(0, max_label, 10))\n",
    "    plt.savefig(os.path.join(save_dir, \"strainwise_usage_plot.jpg\"), dpi=400, bbox_inches='tight', pad_inches=0)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_info_file = \"/Users/dhruvlaad/IIT/DDP/data/finals/label_info.sav\"\n",
    "behaviour_usage_plot(label_info_file, max_label=61, n=30, ns=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-extra",
   "metadata": {},
   "source": [
    "## vignettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video_file(label_info_file, config_file, strain=None):\n",
    "    save_dir = os.path.join(FIG2_SAVEDIR, \"vignette_figure\")\n",
    "    try: os.mkdir(save_dir)\n",
    "    except FileExistsError: pass\n",
    "\n",
    "    with open(label_info_file, \"rb\") as f:\n",
    "        info = joblib.load(f)\n",
    "\n",
    "    if strain:\n",
    "        data = info[strain]\n",
    "    else:\n",
    "        data = info[random.sample(list(info.keys()), 1)[0]]\n",
    "    data = random.sample(data, 1)[0]\n",
    "    metadata, labels = data[\"metadata\"], data[\"labels\"]\n",
    "    \n",
    "    passwd = getpass(\"Box login password: \")\n",
    "    session = ftplib.FTP(\"ftp.box.com\")\n",
    "    session.login(\"ae16b011@smail.iitm.ac.in\", passwd)\n",
    "    data_fname, vid_fname = get_video_and_keypoint_data(session, metadata, save_dir)\n",
    "    data_fname, vid_fname = os.path.join(save_dir, data_fname), os.path.join(save_dir, vid_fname)\n",
    "    \n",
    "    conf, pos = process_h5py_data(h5py.File(os.path.join(save_dir, data_fname)))\n",
    "    data = bsoid_format(conf, pos)\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    fdata, perc_filt = likelihood_filter_no_hour_trim(data, fps=config[\"fps\"], conf_threshold=0.3, bodyparts=config[\"bodyparts\"])\n",
    "\n",
    "    shape = fdata['x'].shape\n",
    "    print(f'Preprocessed {shape} data, with {round(perc_filt, 2)}% data filtered')\n",
    "    \n",
    "    os.remove(data_fname)\n",
    "    data_fname = data_fname.replace(\".csv\", \".pkl\")\n",
    "    with open(data_fname, \"wb\") as f:\n",
    "        joblib.dump(fdata, f)\n",
    "\n",
    "    metadata[\"data_fname\"] = data_fname\n",
    "    metadata[\"vid_fname\"] = vid_fname\n",
    "    metadata[\"labels\"] = labels\n",
    "\n",
    "    with open(os.path.join(save_dir, \"metadata.pkl\"), \"wb\") as f:\n",
    "        joblib.dump(metadata, f)\n",
    "    \n",
    "def save_frames_and_loc_data(behaviour_idx, min_bout_len, fps, n=10):\n",
    "    save_dir = os.path.join(FIG2_SAVEDIR, \"vignette_figure\")\n",
    "    try: os.mkdir(save_dir)\n",
    "    except FileExistsError: pass\n",
    "     \n",
    "    min_bout_len = min_bout_len * fps // 1000\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"metadata.pkl\"), \"rb\") as f:\n",
    "        metadata = joblib.load(f)\n",
    "    video_file = os.path.join(save_dir, metadata[\"vid_fname\"])\n",
    "    with open(os.path.join(save_dir, metadata[\"data_fname\"]), \"rb\") as f:\n",
    "        fdata = joblib.load(f)\n",
    "    labels = metadata[\"labels\"]\n",
    "\n",
    "    save_dir = os.path.join(save_dir, f\"phenotype_{behaviour_idx}_clips\")\n",
    "    try: os.mkdir(save_dir)\n",
    "    except FileExistsError: pass\n",
    "\n",
    "    i, locs = 0, []\n",
    "\n",
    "    while i < len(labels):\n",
    "        if labels[i] == behaviour_idx:\n",
    "            j = i + 1\n",
    "            while j < len(labels) and (labels[j] == labels[i]):\n",
    "                j += 1\n",
    "            \n",
    "            if (j - i + 1) >= min_bout_len:\n",
    "                locs.append([i, j, (j - i + 1)])\n",
    "\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    locs = sorted(locs, key=lambda x: x[-1], reverse=True)[:n]\n",
    "    locs = sorted(locs, key=lambda x: x[0])\n",
    "\n",
    "    count, i = 0, 0\n",
    "    video = cv2.VideoCapture(video_file)\n",
    "    success, image = video.read()\n",
    "    while success and locs:\n",
    "        if i == locs[0][0]:\n",
    "                clip_dir = os.path.join(save_dir, f\"clips_{count}\")\n",
    "            try: os.mkdir(clip_dir)\n",
    "            except FileExistsError: pass\n",
    "\n",
    "            k = 0\n",
    "            while success and i <= locs[0][1]:\n",
    "                cv2.imwrite(os.path.join(clip_dir, f\"phenotype_{behaviour_idx}_frame_{k}.jpg\"), image)\n",
    "                success, image = video.read()\n",
    "                k, i = k + 1, i + 1\n",
    "\n",
    "\n",
    "            start_idx, end_idx = locs[0][:-1]\n",
    "            x, y = fdata['x'][start_idx:end_idx], fdata['y'][start_idx:end_idx]\n",
    "\n",
    "            with open(os.path.join(clip_dir, \"keypoint_data.pkl\"), \"wb\") as f:\n",
    "                joblib.dump([x, y], f)\n",
    "            \n",
    "            del locs[0]\n",
    "            count += 1\n",
    "        else:\n",
    "            i += 1\n",
    "            success, image = video.read()\n",
    "\n",
    "def skeletal_plot(ax, x, y, weight):\n",
    "    HEAD, BASE_NECK, CENTER_SPINE, HINDPAW1, HINDPAW2, BASE_TAIL, MID_TAIL, TIP_TAIL = np.arange(8)\n",
    "    link_connections = ([BASE_TAIL, CENTER_SPINE],\n",
    "                        [CENTER_SPINE, BASE_NECK],\n",
    "                        [BASE_NECK, HEAD],\n",
    "                        [BASE_TAIL, HINDPAW1], [BASE_TAIL, HINDPAW2],\n",
    "                        [BASE_TAIL, MID_TAIL],\n",
    "                        [MID_TAIL, TIP_TAIL])\n",
    "    \n",
    "    cmap = mpl.cm.get_cmap(\"YlGnBu\")\n",
    "    for link in link_connections:\n",
    "        h, t = link\n",
    "        ax.plot([x[h], x[t]], [y[h], y[t]], linewidth=3, color=cmap(weight), alpha=0.5)\n",
    "\n",
    "    for i in np.arange(8):\n",
    "        ax.scatter(x[i], y[i], s=20, color=cmap(weight), alpha=0.5)\n",
    "    \n",
    "    return ax, cmap\n",
    "\n",
    "def make_vignettes(behaviour_idx, clip_no, idxs, ske_idxs, weights, img_crop=None, skeletal_crop=None, deg=0):\n",
    "    assert sum(weights) == 1, \"weights must sum to 1\"\n",
    "    \n",
    "    save_dir = os.path.join(FIG2_SAVEDIR, \"vignette_figure\")\n",
    "    try os.mkdir(save_dir)\n",
    "    except FileExistsError: pass\n",
    "    \n",
    "    frame_dir = os.path.join(save_dir, f\"phenotype_{behaviour_idx}_clips\", f\"clips_{clip_no}\")\n",
    "    frames = [os.path.join(frame_dir, f) for f in os.listdir(frame_dir) if f.endswith(\".jpg\")]\n",
    "    frames.sort(key=lambda x: int(x.split('_')[-1][:-4]))\n",
    "\n",
    "    behaviour = frame_dir.split('/')[-3].split(\"_\")[0]\n",
    "\n",
    "    frames = [cv2.imread(frames[i]) for i in idxs]\n",
    "\n",
    "    img = sum(f * w for f, w in zip(frames, weights)) / 255.0\n",
    "    img = ndimage.rotate(img, deg)\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=2, gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "\n",
    "    ax[0].imshow(img, extent=[0, img.shape[0], 0, img.shape[1]])\n",
    "    ax[1].imshow(img, extent=[0, img.shape[0], 0, img.shape[1]])\n",
    "\n",
    "    with open(os.path.join(frame_dir, \"keypoint_data.pkl\"), \"rb\") as f:\n",
    "        x, y = joblib.load(f)\n",
    "\n",
    "    for i, idx in enumerate(ske_idxs):\n",
    "        wt = (idx - min(ske_idxs))/(max(ske_idxs) - min(ske_idxs))\n",
    "        ax[1], cmap = skeletal_plot(ax[1], x[idx], y[idx], wt)\n",
    "\n",
    "    ax[1].set_xlim(ax[0].get_xlim())\n",
    "    ax[1].set_ylim(ax[0].get_ylim())\n",
    "    \n",
    "    cax = fig.add_axes([0.57, 0.90, 0.15, 0.05])\n",
    "    cb1 = mpl.colorbar.ColorbarBase(cax, cmap=cmap, orientation=\"horizontal\", ticks=[0.0, 1.0])\n",
    "    cax.set_xticks([0.0, 1.0])\n",
    "    cax.set_xticklabels([\"start\", \"end\"])\n",
    "    cax.tick_params(axis=u'both', which=u'both',length=0)\n",
    "\n",
    "    for _, spine in cax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    for (ax_, lims) in zip(ax, [img_crop, skeletal_crop]):\n",
    "        ax_.set_xlim(lims[0])\n",
    "        ax_.set_ylim(lims[1])\n",
    "\n",
    "    for ax_ in ax:\n",
    "        ax_.set_xticklabels([])\n",
    "        ax_.set_yticklabels([])\n",
    "        ax_.set_axis_off()\n",
    "        ax_. set_aspect('equal')\n",
    "        ax_.set_anchor('N')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f\"{behaviour}_fig.jpg\"), dpi=400, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_info_file = \"D:/IIT/DDP/data/finals/label_info.sav\"\n",
    "config_file = \"../config/config.yaml\"\n",
    "download_video_file(label_info_file, config_file, strain=\"C57BL/6J\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames_and_loc_data(behaviour_idx=24, min_bout_len=200, fps=30, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-notebook",
   "metadata": {},
   "source": [
    "## Behavioral Metrics Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_stats(stats, group_labels):\n",
    "    stats_df = {\"MouseID\": [], \"Sex\": [], \"Strain\": [], \"NetworkFilename\": []}\n",
    "    \n",
    "    n_phenos = len(stats[list(stats.keys())[0]]) // 3\n",
    "    for i in range(n_phenos):\n",
    "        for met in [\"TD\", \"ABL\", \"NB\"]:\n",
    "            stats_df[f\"{group_labels[i]}-{met}\"] = []\n",
    "        \n",
    "    for key, data in behaviour_stats.items():\n",
    "        mouse_id, sex, strain, nfname = key.split(';')\n",
    "\n",
    "        stats_df[\"MouseID\"].append(mouse_id)\n",
    "        stats_df[\"Sex\"].append(sex)\n",
    "        stats_df[\"Strain\"].append(strain)\n",
    "        stats_df[\"NetworkFilename\"].append(nfname)\n",
    "        \n",
    "        for i in range(MAX_LABEL):\n",
    "            stats_df[f\"Motif-{i}-TD\"].append(data[f\"phenotype_{i}_td\"])\n",
    "            stats_df[f\"Motif-{i}-ABL\"].append(data[f\"phenotype_{i}_abl\"])\n",
    "            stats_df[f\"Motif-{i}-NB\"].append(data[f\"phenotype_{i}_nb\"])\n",
    "    \n",
    "    stats_df = pd.DataFrame.from_dict(stats_df)\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "functional-crawford",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) // 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_behavioral_metrics():\n",
    "    save_dir = os.path.join(FIG3_SAVEDIR, \"behavioural_metrics\")\n",
    "    try: os.mkdir(save_dir)\n",
    "    except FileExistsError: pass\n",
    "    \n",
    "    _, inv_group_map, group_labels = get_group_map(\"/Users/dhruvlaad/IIT/DDP/data/finals/eac_mat.npy\", pvalue=0.53)\n",
    "    stats = bout_stats_for_all_strains(load_label_info(), MAX_LABEL, min_bout_len=200, fps=30, inv_group_map=inv_group_map)\n",
    "\n",
    "    metrics = (\n",
    "        \"Total Duration\",\n",
    "        \"Average Bout Length\",\n",
    "        \"No. of Bouts\"\n",
    "    )\n",
    "\n",
    "    labels = (\n",
    "        \"Total Duration (min)\",\n",
    "        \"Average Bout Length (s)\",\n",
    "        \"No. of Bouts\"\n",
    "    )\n",
    "\n",
    "    save_names = (\"TD\", \"ABL\", \"NB\")\n",
    "    \n",
    "    for \n",
    "    for behaviour in BEHAVIOUR_LABELS.keys():\n",
    "        print(f\"Plotting for behaviour: {behaviour}\")\n",
    "        \n",
    "        df = grouped_stats[behaviour].copy()\n",
    "        gdfs = df.groupby(\"Strain\")\n",
    "        df[\"Total Duration\"] /= 60\n",
    "\n",
    "        df.loc[df.index[df[\"Strain\"].str.contains(\"BTBR\")], \"Strain\"] = r\"BTBR T$^+$ ltpr3$^{tf}$/J\"\n",
    "\n",
    "        fig, axs = plt.subplots(nrows=len(metrics))\n",
    "        for i, met in enumerate(metrics):\n",
    "            metric_data_mean = gdfs[met].mean().sort_values()\n",
    "            order = list(metric_data_mean.index)\n",
    "\n",
    "            sns.stripplot(x='Strain', y=met, data=df, hue='Sex', jitter=False, order=order, ax=axs[i])\n",
    "            axs[i].tick_params(grid_color='gray', grid_alpha=0.3, labelrotation=90, labelsize=8)\n",
    "            axs[i].grid(True)\n",
    "            axs[i].set_ylabel(labels[i], fontsize=12)\n",
    "            axs[i].set_xlabel(None)\n",
    "            axs[i].tick_params(axis='y', labelrotation=0, labelsize=8)\n",
    "            if ylimits[behaviour][met] is not None:\n",
    "                axs[i].set_ylim(ylimits[behaviour][met])\n",
    "\n",
    "            rect_width = 0.5\n",
    "            metric_data_std = gdfs[met].std().loc[order]\n",
    "            for j in range(metric_data_mean.shape[0]):\n",
    "                rect_height, mean = metric_data_std[j], metric_data_mean[j]\n",
    "                rect_bottom_left = (j - (rect_width / 2), max(mean - rect_height, 0))\n",
    "                if mean - rect_height < 0:\n",
    "                    rect_size = (rect_width, mean)\n",
    "                else:\n",
    "                    rect_size = (rect_width, rect_height)\n",
    "                axs[i].add_patch(plt.Rectangle(rect_bottom_left, *rect_size, edgecolor='k', linewidth=1.2, fill=False, zorder=1000))\n",
    "\n",
    "                rect_bottom_left = (j - (rect_width / 2), mean)\n",
    "                rect_size = (rect_width, rect_height)\n",
    "                axs[i].add_patch(plt.Rectangle(rect_bottom_left, *rect_size, edgecolor='k', linewidth=1.2, fill=False, zorder=1000))\n",
    "\n",
    "            axs[i].figure.set_size_inches(10, 1)\n",
    "            axs[i].yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "            if i > 0:\n",
    "                axs[i].get_legend().set_visible(False)\n",
    "\n",
    "        sns.despine(trim=True)\n",
    "        axs[0].legend(loc='upper left', title='Sex', borderpad=0.5)\n",
    "        fig.set_size_inches(10, 10)\n",
    "        plt.subplots_adjust(hspace=0.6)\n",
    "        if '/' in behaviour:\n",
    "            behaviour = behaviour.replace('/', '-')\n",
    "        plt.savefig(os.path.join(save_dir, f\"{behaviour}_plot.jpg\"), dpi=400, pad_inches=0, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e417718f",
   "metadata": {},
   "source": [
    "# Figure 3\n",
    "---\n",
    "## PVE Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac0f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pve_estimates(pve_estimates_file):\n",
    "    with open(pve_estimates_file, 'r') as f:\n",
    "        pve_data = f.readlines()\n",
    "\n",
    "    pve_df = {\"Phenotype\": [], \"PVE\": [], \"PVESE\": [], \"Vg\": [], \"Ve\": [], \"Group\": []}\n",
    "    for data in pve_data[1:]:\n",
    "        phen, pve, pve_se, vg, ve = data.split(',')\n",
    "        pve_df[\"Phenotype\"].append(phen)\n",
    "        pve_df[\"PVE\"].append(float(pve))\n",
    "        pve_df[\"PVESE\"].append(float(pve_se))\n",
    "        pve_df[\"Vg\"].append(float(vg))\n",
    "        pve_df[\"Ve\"].append(float(ve[:-1]))\n",
    "        \n",
    "        if \"abl\" in phen:\n",
    "            group = \"Average Bout Length\"\n",
    "        elif \"td\" in phen:\n",
    "            group = \"Total Duration\"\n",
    "        elif \"nb\" in phen:\n",
    "            group = \"Number of Bouts\"\n",
    "        pve_df[\"Group\"].append(group)\n",
    "        \n",
    "    pve_df = pd.DataFrame.from_dict(pve_df)\n",
    "    pve_df.sort_values(by=\"PVE\", axis=0, inplace=True)\n",
    "    return pve_df\n",
    "\n",
    "def pve_plot(pve_estimates_file):\n",
    "    pve = pve_estimates(pve_estimates_file)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(3, 12))\n",
    "    sns.barplot(x=\"PVE\", y=\"Phenotype\", data=pve, hue=\"Group\", xerr=pve[\"PVESE\"].values, ax=ax, dodge=False)\n",
    "    ax.set_yticklabels([])\n",
    "    plt.legend(loc=\"upper right\", prop={'size': 8})\n",
    "    sns.despine(trim=True, offset=1)\n",
    "    plt.show()\n",
    "    \n",
    "    return pve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02475dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pve_estimates_file = \"/Users/dhruvlaad/IIT/DDP/data/gwas/PVE_GEMMA_estimates.txt\"\n",
    "pve = pve_plot(pve_estimates_file)\n",
    "pve.iloc[-6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-dutch",
   "metadata": {},
   "source": [
    "## Transition Matrix (Hinton Diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinton(matrix, ax=None):\n",
    "    \"\"\"Draw Hinton diagram for visualizing a weight matrix.\"\"\"\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "\n",
    "    ax.patch.set_facecolor('white')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "    max_weight = 0.15\n",
    "    for (x, y), w in np.ndenumerate(matrix):\n",
    "        color = (0.1, 0.44, 0.82)\n",
    "        if w > max_weight:\n",
    "            color = (0.96, 0.26, 0.71)\n",
    "            w = max_weight / 6\n",
    "        size = np.sqrt(np.abs(w) / max_weight)\n",
    "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
    "                             facecolor=color, edgecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "#     ax.autoscale_view(tight=True)\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    b1, = ax.plot([], marker='s', markersize=2.0, linestyle='', color = (0.96, 0.26, 0.71), label=f\"p > {max_weight}\")\n",
    "    b2, = ax.plot([], marker='s', markersize=3.75, linestyle='', color=(0.1, 0.44, 0.82), label=f\"p = {max_weight}\")\n",
    "\n",
    "    leg = ax.legend(handles=[b1, b2], bbox_to_anchor=(0.95, 0.98), title=\"Transition\\nProbability\", borderpad=0.2, handletextpad=0.05, prop={'size': 7})\n",
    "    leg.get_frame().set_linewidth(0.0)\n",
    "    leg.get_title().set_fontsize('8')\n",
    "    return ax\n",
    "\n",
    "def spectral_reordering_tmat(tmat, groups=4):\n",
    "    labels = SpectralClustering(affinity='precomputed', n_clusters=groups).fit_predict(tmat)\n",
    "\n",
    "    groups = [[] for x in range(labels.max()+1)]\n",
    "    for i, x in enumerate(labels):\n",
    "        groups[x].append(i)\n",
    "        \n",
    "    new_ordering = []\n",
    "    for g in groups:\n",
    "        new_ordering.extend(g)\n",
    "    \n",
    "    spectral_tmat = tmat.copy()\n",
    "    spectral_tmat = spectral_tmat[:,new_ordering]\n",
    "    spectral_tmat = spectral_tmat[new_ordering,:]\n",
    "    \n",
    "    return spectral_tmat\n",
    "\n",
    "def hinton_diagram_transition_matrix_plot(label_info_file, max_label, groups=6, strain=None):\n",
    "    with open(label_info_file, \"rb\") as f:\n",
    "        label_info = joblib.load(f)\n",
    "\n",
    "    print(\"Hinton Diagram Plot:\")\n",
    "    print(\"\\t (1) Extracting transition matrices for each animal\")\n",
    "    strain_tmats = transition_matrix_across_strains(label_info, max_label)\n",
    "    del label_info\n",
    "\n",
    "    tmat = np.zeros((max_label, max_label))\n",
    "    if strain is not None:\n",
    "        for mat in strain_tmats[strain]:\n",
    "            tmat += matmat[\"tmat\"]\n",
    "        tmat /= len(strain_tmats[strain])\n",
    "    else:\n",
    "        N = 0\n",
    "        for _, mats in strain_tmats.items():\n",
    "            for mat in mats:\n",
    "                tmat += mat[\"tmat\"]\n",
    "                N += 1\n",
    "\n",
    "        tmat /= N\n",
    "    del strain_tmats\n",
    "\n",
    "    print(f\"\\t (2) Reording transition matrix (groups={groups})\")\n",
    "    tmat = spectral_reordering_tmat(tmat, groups)\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax = hinton(tmat, ax)\n",
    "    sns.despine(trim=True)\n",
    "    \n",
    "    plt.savefig(os.path.join(FIG3_SAVEDIR, \"transition_matrix.jpg\"), dpi=500, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "hinton_diagram_transition_matrix_plot(label_info_file=\"/Users/dhruvlaad/IIT/DDP/data/finals/label_info.sav\", max_label=61, groups=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0749df3",
   "metadata": {},
   "source": [
    "## Behavioral Statemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tnet_from_tmat(tmat, plot_props):        \n",
    "    G = nx.MultiDiGraph()\n",
    "    G.add_nodes_from(range(0, tmat.shape[0]))\n",
    "    \n",
    "    for i in range(tmat.shape[0]):\n",
    "        for j in range(tmat.shape[1]):\n",
    "            if i != j and np.abs(tmat[i,j]) > plot_props[\"min_thresh\"]:\n",
    "                G.add_edge(i, j, weight=tmat[i,j])\n",
    "    \n",
    "    return G\n",
    "        \n",
    "def behavioural_statemap_for_strain(plot_props, tmat=None, prop=None, strain=None, n=None, ns=None):\n",
    "    tmat = get_transition_matrix(strain=strain, n=n, ns=ns) if tmat is None else tmat\n",
    "    prop = get_usage(strain=strain, n=n, ns=ns)[0] if prop is None else prop\n",
    "\n",
    "    G = tnet_from_tmat(tmat, plot_props)\n",
    "    return G, tmat, prop\n",
    "\n",
    "def draw_statemap(G, prop, plot_props, ax=None, savefile=None, pos=None, return_pos=False, legend=False):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    edge_wts, edge_bins = plot_props[\"edge_wgts\"], plot_props[\"edge_bins\"]\n",
    "    widths = [edge_wts[np.digitize([G[u][v][0]['weight']], edge_bins)[0]] for u, v in G.edges()]\n",
    "    alphas = [plot_props[\"alphas\"][np.digitize([G[u][v][0]['weight']], edge_bins)[0]] for u, v in G.edges()]\n",
    "    \n",
    "    edges = [G[u][v][0]['weight'] for u, v in G.edges()]\n",
    "    edge_colors = [tuple([*plot_props[\"color\"], alphas[i]]) for i in range(len(edges))]\n",
    "        \n",
    "    if pos is None:\n",
    "        pos = generate_statemap_layout(G, plot_props)\n",
    "\n",
    "    nx.draw(G, ax=ax, pos=pos, node_size=2000*prop, connectionstyle='bar, fraction = 0.01', edge_color=edge_colors, edgecolors='k', node_color='white', width=widths, arrowsize=5)\n",
    "    if legend:\n",
    "        ax = add_legend(ax, plot_props)\n",
    "    \n",
    "    if savefile is not None:\n",
    "        plt.savefig(os.path.join(FIG3_SAVEDIR, savefile), dpi=400, pad_inches=0, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "def draw_diff_state_plot(graph1, graph2, plot_props, pos=None, ax=None, savefile=None, legend=False):\n",
    "    G1, tmat1, prop1 = graph1\n",
    "    G2, tmat2, prop2 = graph2\n",
    "    \n",
    "    tmatdiff, propdiff = tmat1 - tmat2, prop1 - prop2\n",
    "    Gdiff, _, _ = behavioural_statemap_for_strain(tmat=tmatdiff, prop=propdiff, plot_props=plot_props)\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    edge_wts, edge_bins = plot_props[\"edge_wgts\"], plot_props[\"edge_bins\"]\n",
    "    widths = [edge_wts[np.digitize([Gdiff[u][v][0]['weight']], edge_bins)[0]] for u, v in Gdiff.edges()]\n",
    "    alphas = [plot_props[\"alphas\"][np.digitize([Gdiff[u][v][0]['weight']], edge_bins)[0]] for u, v in Gdiff.edges()]\n",
    "    \n",
    "    edges = [Gdiff[u][v][0]['weight'] for u, v in Gdiff.edges()]\n",
    "    edge_colors = []\n",
    "    for i, e in enumerate(edges):\n",
    "        if e < 0:\n",
    "            color = tuple([*plot_props[\"neg_color\"], alphas[i]])\n",
    "        else:\n",
    "            color = tuple([*plot_props[\"pos_color\"], alphas[i]])\n",
    "        edge_colors.append(color)\n",
    "    \n",
    "    node_colors, edgecolors = [], []\n",
    "    max_node_colors, min_node_colors = [], []\n",
    "    for p in propdiff:\n",
    "        if p < 0:\n",
    "            color = [*plot_props[\"neg_color\"], 0.7]\n",
    "        else:\n",
    "            color = [*plot_props[\"pos_color\"], 0.7]\n",
    "        node_colors.append(tuple(color))\n",
    "        \n",
    "        color[-1] = 1.0\n",
    "        edgecolors.append(tuple(color))\n",
    "        \n",
    "        if p < 0:\n",
    "            max_node_colors.append(color)\n",
    "            min_node_colors.append('k')\n",
    "        else:\n",
    "            max_node_colors.append('k')\n",
    "            min_node_colors.append(color)\n",
    "        \n",
    "    node_sizes = np.vstack((prop1, prop2))\n",
    "\n",
    "    if pos is None:\n",
    "        pos = generate_statemap_layout(Gdiff, plot_props)\n",
    "    if legend:\n",
    "        ax = add_legend(ax, plot_props)\n",
    "        \n",
    "    nx.draw(Gdiff, ax=ax, pos=pos, node_size=2000*node_sizes.max(axis=0), connectionstyle='bar, fraction = 0.01', edge_color=edge_colors, edgecolors=max_node_colors, node_color=node_colors, width=widths, arrowsize=5)\n",
    "    nx.draw_networkx_nodes(Gdiff, ax=ax, pos=pos, node_size=2000*node_sizes.min(axis=0), node_color=\"white\", edgecolors=min_node_colors)\n",
    "    \n",
    "    if savefile is not None:\n",
    "        plt.savefig(os.path.join(FIG3_SAVEDIR, savefile), dpi=400, pad_inches=0, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_props = {\n",
    "    \"edge_wgts\": [0, 0.5, 1.5, 5],\n",
    "    \"edge_bins\": [0.01, 0.045, 0.075],\n",
    "    \"color\": (0.071, 0.247, 0.631),\n",
    "    \"alphas\": [0, 0.1, 0.5, 0.7],\n",
    "    \"min_thresh\": 0.0025\n",
    "}\n",
    "\n",
    "diff_plot_props = {\n",
    "    \"edge_wgts\": [2.5, 1.5, 0.5, 0, 0.5, 1.5, 2.5],\n",
    "    \"edge_bins\": [-0.05, -0.025, -0.005, 0.005, 0.025, 0.05],\n",
    "    \"pos_color\": (0.071, 0.247, 0.631),\n",
    "    \"neg_color\": (0.741, 0.106, 0.059),\n",
    "    \"alphas\": [0.7, 0.5, 0.1, 0, 0.1, 0.5, 0.7],\n",
    "    \"min_thresh\": 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c42c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gall, tmatall, propall = behavioural_statemap_for_strain(plot_props=plot_props)\n",
    "pos = draw_statemap(Gall, propall, plot_props, return_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5080db10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strain1, strain2 = \"C57BL/6J\", \"MSM/MsJ\"\n",
    "G1, tmat1, prop1 = behavioural_statemap_for_strain(strain=strain1, plot_props=plot_props, n=25, ns=500)\n",
    "G2, tmat2, prop2 = behavioural_statemap_for_strain(strain=strain2, plot_props=plot_props, n=15, ns=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d78b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statemap_layout(tmat, thresh):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(0, tmat.shape[0]))\n",
    "    \n",
    "    for i, j in combinations(range(tmat.shape[0]), 2):\n",
    "        if i != j and abs(tmat[i,j]) > thresh:\n",
    "            wt = 1 if tmat[i,j] > 0 else -1\n",
    "            G.add_edge(i, j, weight=wt)\n",
    "    pos = nx.spectral_layout(G)\n",
    "    \n",
    "    for u, v in G.edges():\n",
    "        if G[u][v][\"weight\"] != 0:\n",
    "            G[u][v][\"weight\"] = max(abs(tmat[u,v]), abs(tmat[v,u]))\n",
    "            \n",
    "    pos = nx.spring_layout(G, pos=pos, k=0.1)\n",
    "    return pos\n",
    "\n",
    "def add_legend(ax, plot_props):\n",
    "    import matplotlib.patches as mpatches\n",
    "    from matplotlib.lines import Line2D\n",
    "    if \"pos_color\" in plot_props:\n",
    "        colors = [plot_props[\"neg_color\"], plot_props[\"neg_color\"], plot_props[\"pos_color\"], plot_props[\"pos_color\"]]\n",
    "        labels = [f\"-{round(abs(val)*100, 1)}%\" for val in plot_props[\"edge_bins\"][:2]]\n",
    "        labels.extend([f\"+{round(val*100, 1)}%\" for val in plot_props[\"edge_bins\"][4:]])\n",
    "        sizes = [5, 1.5, 1.5, 5]\n",
    "        correction = [0, 0.9, 0.9, 0]\n",
    "    else:\n",
    "        colors = [plot_props[\"color\"]] * 3\n",
    "        labels = [f\"+{round(val*100, 1)}%\" for val in plot_props[\"edge_bins\"]]\n",
    "        sizes = [0.5, 2, 5]\n",
    "    \n",
    "    colors = [list(c) + [1.0] for c in colors]\n",
    "    patches = [Line2D([0], [0], label=labels[i],color=colors[i],linewidth=sizes[i]) for i in range(len(labels))]\n",
    "    leg = ax.legend(handles=patches, bbox_to_anchor=(0.05, 0.75), loc='lower left', labelspacing=0.25, frameon=False, title='Transition\\nProbability', prop={'size': 7})\n",
    "    plt.setp(leg.get_title(),fontsize=7)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba7c1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pos = generate_statemap_layout(tmat1 - tmat2, 0.01)\n",
    "# draw_statemap(G1, prop1, plot_props, savefile=\"c57bl6j_statemap.jpg\", return_pos=True, legend=True, pos=pos)\n",
    "# draw_statemap(G2, prop2, plot_props, savefile=\"casteij_statemap.jpg\", pos=pos)\n",
    "draw_diff_state_plot([G1, tmat1, prop1], [G2, tmat2, prop2], plot_props=diff_plot_props, savefile=\"diff_statemap.jpg\", pos=pos, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eeefca",
   "metadata": {},
   "source": [
    "## Statemap Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43bb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_plot(strain1, strain2, ax):\n",
    "    G1, tmat1, prop1 = behavioural_statemap_for_strain(strain=strain1, plot_props=plot_props)\n",
    "    G2, tmat2, prop2 = behavioural_statemap_for_strain(strain=strain2, plot_props=plot_props)\n",
    "        \n",
    "    pos = nx.spring_layout(G1, pos=nx.spectral_layout(G1))\n",
    "    draw_diff_state_plot([G1, tmat1, prop1], [G2, tmat2, prop2], plot_props=diff_plot_props, pos=pos, savefile=\"diff_statemap.jpg\", ax=ax)\n",
    "    return ax\n",
    "\n",
    "def statemap_scatter_plot(nrows=7, ncols=4):\n",
    "    strains = (load_label_info().keys())\n",
    "    \n",
    "    N = int(nrows * ncols)\n",
    "    combs = random.sample(list(combinations(strains, 2)), N)\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(6, 9))\n",
    "    k = 0\n",
    "    pbar = tqdm(total=N)\n",
    "    for i in range(len(axs)):\n",
    "        for j in range(len(axs[0])):\n",
    "            diff_plot(*combs[k], axs[i][j])\n",
    "            k += 1\n",
    "            pbar.update(1)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c628245",
   "metadata": {},
   "outputs": [],
   "source": [
    "statemap_scatter_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-discrimination",
   "metadata": {},
   "source": [
    "# Figures for DDP Report\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-january",
   "metadata": {},
   "source": [
    "## Validation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_validaiton_plot():\n",
    "    df = pd.read_csv(\"/Users/dhruvlaad/IIT/DDP/data/finals/cluster_validation.csv\")\n",
    "    \n",
    "    df[\"Correlation\"] = -df[\"Correlation\"]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(2, 3))\n",
    "    data = [df[\"Norm Conn\"].values, df[\"Correlation\"].values]\n",
    "    labels = [\"Conn.\", \"Corr.\"]\n",
    "    \n",
    "    bplot = ax.boxplot(data, showcaps=False, patch_artist=True, flierprops=dict(markersize=2))\n",
    "    \n",
    "    colors = [\"lightblue\", \"pink\"]\n",
    "    for box, col in zip(bplot[\"boxes\"], colors):\n",
    "        box.set_facecolor(col)\n",
    "    ax.legend([bplot[\"boxes\"][0], bplot[\"boxes\"][1]], labels, loc='upper right', prop={'size': 8})\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.tick_params(axis='y', which=\"both\", labelsize=7)\n",
    "    plt.savefig(\"/Users/dhruvlaad/IIT/DDP/data/plots/validation.jpg\", dpi=300)\n",
    "    plt.show()\n",
    "clustering_validaiton_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69665f25",
   "metadata": {},
   "source": [
    "## Multi-Step Sampling Procedure Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.fft import fftshift, ifft2, fft2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def get_maxrange(data):\n",
    "    return np.vstack((data.max(axis=0), data.min(axis=0)))\n",
    "\"\"\"\n",
    "Copied from https://github.com/bermanlabemory/motionmapperpy/blob/master/motionmapperpy/mmutils.py\n",
    "\"\"\"\n",
    "def gencmap():\n",
    "    colors = np.zeros((64, 3))\n",
    "    colors[:21, 0] = np.linspace(1, 0, 21)\n",
    "    colors[20:43, 0] = np.linspace(0, 1, 23)\n",
    "    colors[42:, 0] = 1.0\n",
    "\n",
    "    colors[:21, 1] = np.linspace(1, 0, 21)\n",
    "    colors[20:43, 1] = np.linspace(0, 1, 23)\n",
    "    colors[42:, 1] = np.linspace(1, 0, 22)\n",
    "\n",
    "    colors[:21, 2] = 1.0\n",
    "    colors[20:43, 2] = np.linspace(1, 0, 23)\n",
    "    colors[42:, 2] = 0.0\n",
    "    return mpl.colors.ListedColormap(colors)\n",
    "\n",
    "def findPointDensity(data, sigma, numPoints):\n",
    "    data = MinMaxScaler((-75, 75)).fit_transform(data)\n",
    "    xx = np.linspace(-100, 100, numPoints)\n",
    "    yy = xx.copy()\n",
    "    [XX, YY] = np.meshgrid(xx, yy)\n",
    "    G = np.exp(-0.5 * (np.square(XX) + np.square(YY)) / np.square(sigma))\n",
    "    Z = np.histogramdd(data, bins=[xx, yy])[0]\n",
    "    Z = Z / np.sum(Z)\n",
    "    Z = np.pad(Z, ((0, 1), (0, 1)), mode='constant', constant_values=((0, 0), (0, 0)))\n",
    "    density = fftshift(np.real(ifft2(np.multiply(fft2(G), fft2(Z))))).T\n",
    "    density[density < 0] = 0\n",
    "    return density\n",
    "\n",
    "def plot_density(data, sigma, num_points, ax=None):\n",
    "    density = findPointDensity(data, sigma=sigma, numPoints=num_points)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6,6))\n",
    "    else:\n",
    "        fig = plt.gcf()\n",
    "    ax.imshow(density, cmap=gencmap())\n",
    "    sns.despine(top=True, bottom=True, left=True, right=True)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_yticklabels([])\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f68aae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from openTSNE import TSNE, affinity, initialization\n",
    "def get_tsne_embedding(data):\n",
    "    data = StandardScaler().fit_transform(data)\n",
    "    affinities = affinity.Multiscale(data, perplexities=[50, 500], metric=\"cosine\", n_jobs=10)\n",
    "    init = initialization.pca(data)\n",
    "    embedding = TSNE(n_jobs=10).fit(affinities=affinities, initialization=init)\n",
    "    return embedding\n",
    "\n",
    "def multistepplot():\n",
    "    strain = \"C57BL/6J\"\n",
    "    bsoid = BSOID(\"../config/config.yaml\")\n",
    "    feats = random.sample(bsoid.load_features(collect=False)[strain], 4)\n",
    "    \n",
    "    embeddings = [get_tsne_embedding(f) for f in feats]\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(8,8))\n",
    "    k = 0\n",
    "    for i in range(len(axs)):\n",
    "        for j in range(len(axs[0])):\n",
    "            _, axs[i][j] = plot_density(embeddings[k], sigma=3.5, num_points=1000, ax=axs[i][j])\n",
    "            k += 1\n",
    "    \n",
    "    plt.show()\n",
    "multistepplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-laser",
   "metadata": {},
   "source": [
    "## Macro Grouping plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_eac_plot(eac_mat, pvalue=None):    \n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    im = ax.imshow(eac_mat, cmap=\"gray\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_yticklabels([])\n",
    "    \n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = fig.colorbar(im, cax=cax)\n",
    "    cbar.set_ticks([0, 0.99])\n",
    "    cbar.set_ticklabels([0, 1.0])\n",
    "    cbar.ax.set_ylabel(\"Normalized Dissimilarity Index\", labelpad=-1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(DDP_SAVEDIR, \"eac.jpg\"), dpi=500, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.show()\n",
    "    \n",
    "    Z = scipy.cluster.hierarchy.linkage(squareform(eac_mat), method=\"single\")\n",
    "    \n",
    "    if pvalue is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 9))\n",
    "        ddata = scipy.cluster.hierarchy.dendrogram(Z, ax=ax)\n",
    "        ylocs = []\n",
    "        for i, d in zip(ddata['icoord'], ddata['dcoord']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            ax.plot([0, x], [y, y], '--g')\n",
    "            ylocs.append(y)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        pvalue = float(input(\"p-value for choosing split: \"))\n",
    "    \n",
    "    group_labels = scipy.cluster.hierarchy.fcluster(Z, t=pvalue, criterion=\"distance\")\n",
    "    cmap = plt.cm.tab20b(np.unique(group_labels) / group_labels.max())\n",
    "    scipy.cluster.hierarchy.set_link_color_palette([mpl.colors.rgb2hex(rgb[:3]) for rgb in cmap])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    scipy.cluster.hierarchy.dendrogram(Z, ax=ax, color_threshold=pvalue)\n",
    "    xlim = ax.get_xlim()\n",
    "    ax.plot([0, xlim[1]], [pvalue, pvalue], '--r')\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    \n",
    "    sns.despine(top=True, right=True, trim=True)\n",
    "    plt.savefig(os.path.join(DDP_SAVEDIR, \"dendogram.jpg\"), dpi=500, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-turning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pvalue = 0.53\n",
    "cluster_eac_plot(eac_mat=np.load(\"/Users/dhruvlaad/IIT/DDP/data/finals/eac_mat.npy\"), pvalue=pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-intent",
   "metadata": {},
   "source": [
    "## Group Strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-dodge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "from scipy.spatial import ConvexHull\n",
    "from adjustText import adjust_text\n",
    "\n",
    "@numba.njit(fastmath=True)\n",
    "def symmetric_kl(x, y, z=1e-11):  # pragma: no cover\n",
    "    \"\"\"\n",
    "    symmetrized KL divergence between two probability distributions\n",
    "    ..math::\n",
    "        D(x, y) = \\frac{D_{KL}\\left(x \\Vert y\\right) + D_{KL}\\left(y \\Vert x\\right)}{2}\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    x_sum = 0.0\n",
    "    y_sum = 0.0\n",
    "    kl1 = 0.0\n",
    "    kl2 = 0.0\n",
    "\n",
    "    for i in range(n):\n",
    "        x[i] += z\n",
    "        x_sum += x[i]\n",
    "        y[i] += z\n",
    "        y_sum += y[i]\n",
    "\n",
    "    for i in range(n):\n",
    "        x[i] /= x_sum\n",
    "        y[i] /= y_sum\n",
    "\n",
    "    for i in range(n):\n",
    "        kl1 += x[i] * np.log(x[i] / y[i])\n",
    "        kl2 += y[i] * np.log(y[i] / x[i])\n",
    "\n",
    "    return (kl1 + kl2) / 2\n",
    "\n",
    "def make_filled_clusters(embedding, labels, ax, colors):\n",
    "    for lab in np.unique(labels):\n",
    "        points = embedding[labels == lab]\n",
    "        hull = ConvexHull(points)\n",
    "        x_hull = np.append(points[hull.vertices,0],\n",
    "                       points[hull.vertices,0][0])\n",
    "        y_hull = np.append(points[hull.vertices,1],\n",
    "                           points[hull.vertices,1][0])\n",
    "        ax.fill(x_hull, y_hull, alpha=0.3, c=np.array(colors)[labels == lab][0])\n",
    "    return ax\n",
    "\n",
    "def draw_lines_to_center(embedding, labels, ax, colors):\n",
    "    colors = np.array(colors)\n",
    "    for lab in np.unique(labels):\n",
    "        points = embedding[labels == lab]\n",
    "        mean = points.mean(axis=0)\n",
    "        for i in range(points.shape[0]):\n",
    "            ax.plot([mean[0], points[i,0]], [mean[1], points[i,1]], c=colors[labels == lab][0], linewidth=1)\n",
    "    return ax\n",
    "\n",
    "def get_sim_mat(tmats=None, props=None):\n",
    "    if tmats is None or props is None:\n",
    "        label_info = load_label_info()\n",
    "    \n",
    "    if tmats is None:\n",
    "        tmats = transition_matrix_across_strains(label_info, MAX_LABEL)\n",
    "        for strain in tmats.keys():\n",
    "            tmats[strain] = np.array([data[\"tmat\"] for data in tmats[strain]])\n",
    "    if props is None:\n",
    "        props = proportion_usage_across_strains(label_info, MAX_LABEL)\n",
    "        for strain in props.keys():\n",
    "            props[strain] = np.vstack([data[\"prop\"] for data in props[strain]])\n",
    "    \n",
    "    idx2strain = {i: strain for i, strain in enumerate(list(props.keys()))}\n",
    "    n = len(idx2strain)\n",
    "\n",
    "    X = []\n",
    "    for i in range(n):\n",
    "        strain = idx2strain[i]\n",
    "        strain_props = bootstrap_estimate(props[strain], n=15, ns=500)[0]\n",
    "        strain_tmats = bootstrap_estimate(tmats[strain], n=15, ns=1000)[0]\n",
    "        X.append(np.hstack((strain_props, strain_tmats.flatten())))\n",
    "    X = np.vstack(X)\n",
    "    \n",
    "    M = np.zeros((n, n))\n",
    "    for i, j in combinations(range(n), 2):\n",
    "        M[i,j] = M[j,i] = symmetric_kl(X[i], X[j])\n",
    "    \n",
    "    return M, idx2strain\n",
    "\n",
    "def cluster_strains(M):\n",
    "    embedding = umap.UMAP(n_components=2, min_dist=0.0, n_neighbors=10, metric=\"precomputed\").fit_transform(M)\n",
    "    labels = cluster_with_hdbscan(embedding, verbose=False, min_samples=1, prediction_data=True, cluster_range=[5, 9])[2]\n",
    "#     print(f\"Groups: {labels.max() + 1}\")\n",
    "    return embedding, labels\n",
    "\n",
    "def ea_runs(n):\n",
    "    label_info = load_label_info()\n",
    "    all_tmats = transition_matrix_across_strains(label_info, MAX_LABEL)\n",
    "    all_props = proportion_usage_across_strains(label_info, MAX_LABEL)\n",
    "    \n",
    "    for strain in all_tmats.keys():\n",
    "        all_tmats[strain] = np.array([data[\"tmat\"] for data in all_tmats[strain]])\n",
    "        all_props[strain] = np.vstack([data[\"prop\"] for data in all_props[strain]])\n",
    "        \n",
    "    labels = []\n",
    "    for _ in tqdm(range(n)):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            labels.append(cluster_strains(get_sim_mat(all_tmats, all_props)[0])[1])\n",
    "        \n",
    "    eac_mat = np.array([coassociation_matrix(lab) for lab in labels])\n",
    "    eac_mat = eac_mat.sum(axis=0) / eac_mat.shape[0]\n",
    "    dissim_mat = np.abs(eac_mat.max() - eac_mat)\n",
    "    return dissim_mat\n",
    "\n",
    "def eac_group(dissim_mat, idx2strain, pvalue=None):\n",
    "    Z = scipy.cluster.hierarchy.linkage(squareform(dissim_mat), method=\"single\")  \n",
    "    if pvalue is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 9))\n",
    "        ddata = scipy.cluster.hierarchy.dendrogram(Z, ax=ax, orientation=\"left\")\n",
    "        ylocs = []\n",
    "        for i, d in zip(ddata['icoord'], ddata['dcoord']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            ax.plot([0, x], [y, y], '--g')\n",
    "            ylocs.append(y)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        pvalue = float(input(\"p-value for choosing split: \"))\n",
    "    \n",
    "    group_labels = scipy.cluster.hierarchy.fcluster(Z, t=pvalue, criterion=\"distance\")\n",
    "    cmap = plt.cm.tab20(np.unique(group_labels))\n",
    "    scipy.cluster.hierarchy.set_link_color_palette([mpl.colors.rgb2hex(rgb[:3]) for rgb in cmap])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(3, 8))\n",
    "    scipy.cluster.hierarchy.dendrogram(Z, ax=ax, color_threshold=pvalue, orientation=\"left\")\n",
    "    ylim = ax.get_ylim()\n",
    "    ax.plot([pvalue, pvalue], [0, ylim[1]], '--r')\n",
    "    \n",
    "    ax.set_yticklabels([idx2strain[int(lab.get_text())] for lab in ax.get_yticklabels()])\n",
    "#     ax.tick_params(axis=\"x\", which=\"both\", labelsize=3.75, pad=0)\n",
    "\n",
    "    sns.despine(top=True, right=True, left=True, trim=True)\n",
    "    plt.savefig(os.path.join(DDP_SAVEDIR, \"strainclusters_dendogram.jpg\"), dpi=500, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.show()\n",
    "    \n",
    "    return group_labels\n",
    "        \n",
    "def annotated_plot(embedding, labels, idx2strain):\n",
    "#     M, idx2strain = get_sim_mat()\n",
    "#     embedding = umap.UMAP(n_components=2, min_dist=0.0, n_neighbors=10, metric=\"precomputed\").fit_transform(M)\n",
    "    \n",
    "    \n",
    "    for idx, strain in idx2strain.items():\n",
    "        if \"BTBR\" in strain:\n",
    "            idx2strain[idx] = \"$\\mathregular{BTBR T^+ Itpr3^{tf}/J}$\"\n",
    "\n",
    "    cmap = mpl.cm.get_cmap(\"tab20\")\n",
    "    colors = [cmap(lab) for lab in labels]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.scatter(embedding[:,0], embedding[:,1], s=3, c=colors)\n",
    "    ax = make_filled_clusters(embedding, labels, ax, colors)\n",
    "    ax = draw_lines_to_center(embedding, labels, ax, colors)\n",
    "\n",
    "    texts = []\n",
    "    for i in range(embedding.shape[0]):\n",
    "        texts.append(ax.text(*embedding[i], idx2strain[i], fontsize=6, color=colors[i]))\n",
    "\n",
    "    sns.despine(top=True, bottom=True, left=True, right=True)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_yticklabels([])\n",
    "    \n",
    "    adjust_text(texts)\n",
    "    plt.savefig(os.path.join(DDP_SAVEDIR, \"strainclusters.jpg\"), dpi=500, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "dissim_mat = ea_runs(n=300)\n",
    "np.save(\"/Users/dhruvlaad/IIT/DDP/data/finals/straincluster_eac.npy\", dissim_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = eac_group(dissim_mat, idx2strain, pvalue=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, idx2strain = get_sim_mat()\n",
    "embedding = umap.UMAP(n_components=2, min_dist=0.0, n_neighbors=10, metric=\"precomputed\").fit_transform(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_plot(embedding, labels, idx2strain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
