{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "major-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import sys\n",
    "import random\n",
    "sys.path.insert(0, \"D:/IIT/DDP/DDP/B-SOID\")\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from new_clustering import *\n",
    "from BSOID.similarity import *\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indian-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../../data/2clustering\"\n",
    "with open(os.path.join(data_dir, \"strainwise_labels.sav\"), \"rb\") as f:\n",
    "    feats, embedding, labels = joblib.load(f)\n",
    "with open(os.path.join(data_dir, \"pairwise_sim.sav\"), \"rb\") as f:\n",
    "    sim, thresh = joblib.load(f)\n",
    "feats = collect_strainwise_feats(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-butter",
   "metadata": {},
   "source": [
    "# Try Similarity Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = collect_strainwise_clusters(feats, labels, embedding, thresh=0.9)\n",
    "\n",
    "strain2idx = {}\n",
    "for cluster_idx in clusters.keys():\n",
    "    strain = cluster_idx.split(':')[0]\n",
    "    if strain in strain2idx:\n",
    "        strain2idx[strain].append(cluster_idx)\n",
    "    else:\n",
    "        strain2idx[strain] = [cluster_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "@njit\n",
    "def cdist2val(D):\n",
    "    m,n = D.shape\n",
    "    result = 0.0\n",
    "    for i in range(m):\n",
    "        for j in range(i,n):\n",
    "            result += D[i,j]\n",
    "    return result / (m * n)\n",
    "\n",
    "from scipy.spatial.distance import cdist, directed_hausdorff\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "def cluster_similarity(cluster1, cluster2):\n",
    "    X1, X2 = np.copy(cluster1[\"feats\"]), np.copy(cluster2[\"feats\"])\n",
    "    y = np.hstack((np.zeros((X1.shape[0],)), np.ones((X2.shape[0],))))\n",
    "    X = np.vstack((X1, X2))\n",
    "\n",
    "#     model = LinearDiscriminantAnalysis().fit(X, y)\n",
    "#     X1proj, X2proj = model.transform(X1), model.transform(X2)\n",
    "    \n",
    "#     D = cdist(X1proj, X2proj, metric=\"mahalanobis\")\n",
    "    return calinski_harabasz_score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "strain2clusters = {}\n",
    "for clusterid in clusters.keys():\n",
    "    strain = clusterid.split(':')[0]\n",
    "    if strain in strain2clusters:\n",
    "        strain2clusters[strain].append(clusterid)\n",
    "    else:\n",
    "        strain2clusters[strain] = [clusterid]\n",
    "\n",
    "strain = random.sample(list(strain2clusters.keys()), 1)[0]\n",
    "wthin_strain = Parallel(n_jobs=6)(delayed(cluster_similarity)(clusters[clusteridx1], clusters[clusteridx2]) for clusteridx1, clusteridx2 in combinations(strain2clusters[strain], 2)) \n",
    "\n",
    "strain1, strain2 = random.sample(list(strain2clusters.keys()), 2)\n",
    "bween_strain = []\n",
    "for clusteridx1 in strain2clusters[strain1]:\n",
    "    for clusteridx2 in strain2clusters[strain2]:\n",
    "        bween_strain.append(cluster_similarity(clusters[clusteridx1], clusters[clusteridx2]))\n",
    "wthin_strain, bween_strain = np.array(wthin_strain), np.array(bween_strain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(wthin_strain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(bween_strain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-completion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_random_strain(thresh):\n",
    "    while True:\n",
    "        strain = random.sample(list(strain2idx.keys()), 1)[0]\n",
    "        X = [clusters[cluster_idx][\"feats\"] for cluster_idx in strain2idx[strain]]\n",
    "        y = [i * np.ones((x.shape[0],)) for i, x in enumerate(X)]\n",
    "        X, y = np.vstack(X), np.hstack(y)\n",
    "        counts = np.unique(y, return_counts=True)[1]\n",
    "        prop = [x / y.size for x in counts]\n",
    "        entropy_ratio = -sum(p * np.log2(p) for p in prop) / max_entropy(len(prop))\n",
    "        if entropy_ratio >= thresh:\n",
    "            print(f\"found: {strain} with entropy: {entropy_ratio}\")\n",
    "            break\n",
    "    \n",
    "    mapper = umap.UMAP(min_dist=0.0, n_neighbors=500, n_components=2, densmap=True).fit(StandardScaler().fit_transform(X))\n",
    "    embed = mapper.embedding_\n",
    "    _, _, glabels, _ = cluster_with_hdbscan(embed, [0.5, 1.0, 11], {\"prediction_data\": True, \"min_samples\": 1})\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(embed[:,0], embed[:,1], c=glabels, cmap=\"Spectral\", s=0.2, alpha=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_strain(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-means",
   "metadata": {},
   "source": [
    "# Trim identified clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy_ratio_for_strain(strain, clusters):\n",
    "    counts = []\n",
    "    for cluster_id, data in clusters.items():\n",
    "        if cluster_id.split(':')[0] == strain:\n",
    "            counts.append(data[\"feats\"].shape[0])\n",
    "    \n",
    "    prop = [x / sum(counts) for x in counts]\n",
    "    entropy_ratio = -sum(p * np.log2(p) for p in prop) / max_entropy(len(counts))\n",
    "\n",
    "    return entropy_ratio\n",
    "\n",
    "def trim_clusters(clusters, sim_mat, thresh):\n",
    "    # find strains below threshold\n",
    "    strains = list(set([cluster_id.split(':')[0] for cluster_id in clusters.keys()]))\n",
    "    remove_strains = [strain for strain in strains if get_entropy_ratio_for_strain(strain, clusters) < thresh]\n",
    "    \n",
    "    # find cluster idxs to be retained/removed\n",
    "    retain_k, retain_cluster_ids = [], []\n",
    "    for cluster_id in clusters.keys():\n",
    "        strain, _, k = cluster_id.split(':')\n",
    "        if strain not in remove_strains:\n",
    "            retain_k.append(int(k))\n",
    "            retain_cluster_ids.append(cluster_id)\n",
    "    \n",
    "    print(f\"Retained {len(retain_cluster_ids)} out of {len(clusters)} clusters\")\n",
    "    \n",
    "    sim_mat = sim_mat[:,retain_k]\n",
    "    sim_mat = sim_mat[retain_k,:]\n",
    "    \n",
    "    idxmap = {k: i for i, k in enumerate(sorted(retain_k))}\n",
    "    clusters = {cluster_id: clusters[cluster_id] for cluster_id in retain_cluster_ids}\n",
    "    new_clusters = {}\n",
    "    for cluster_id, data in clusters.items():\n",
    "        strain, idx, k = cluster_id.split(':')\n",
    "        new_clusters[f\"{strain}:{idx}:{idxmap[int(k)]}\"] = data\n",
    "\n",
    "    return sim_mat, new_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-safety",
   "metadata": {},
   "source": [
    "# Grouping Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_group_clusters(sim_mat):\n",
    "    mapper = umap.UMAP(min_dist=0.0, n_neighbors=50, n_components=2).fit(sim_mat)\n",
    "    assgn, _, glabels, _ = cluster_with_hdbscan(mapper.embedding_, [1.5, 3], HDBSCAN_PARAMS)\n",
    "    \n",
    "    embed = mapper.embedding_[assgn >= 0]\n",
    "    plt.scatter(embed[:,0], embed[:,1], c=glabels[assgn >= 0], s=5, cmap=\"Spectral\")\n",
    "    plt.show(){}\n",
    "    \n",
    "    return glabels\n",
    "\n",
    "def same_strain_grouping_frac(glabels, clusters):\n",
    "    cluster2group = {k: lab for k, lab in enumerate(glabels)}\n",
    "    group_frac = {}\n",
    "    for cluster_id in clusters.keys():\n",
    "        strain, _, k = cluster_id.split(':')\n",
    "        if strain in group_frac:\n",
    "            group_frac[strain].append(cluster2group[int(k)])\n",
    "        else:\n",
    "            group_frac[strain] = [cluster2group[int(k)]]\n",
    "    group_frac = {strain: round(np.unique(labs).size / len(labs), 2) for strain, labs in group_frac.items()}\n",
    "    return group_frac\n",
    "\n",
    "def avg_group_sim(glabels, clusters, sim_mat):\n",
    "    group_sim = {}\n",
    "    for k, lab in enumerate(glabels):\n",
    "        if lab in group_sim:\n",
    "            group_sim[lab].append(k)\n",
    "        else:\n",
    "            group_sim[lab] = [k]\n",
    "    \n",
    "    within_group_sim = {}\n",
    "    for group, cluster_idx in group_sim.items():\n",
    "        within_group_sim[group] = np.array([sim_mat[i,j] for i, j in combinations(cluster_idx, 2)]).mean()\n",
    "    \n",
    "    n = glabels.max() + 1\n",
    "    between_group_sim = np.zeros((n, n))\n",
    "    for i, j in combinations(list(group_sim.keys()), 2):\n",
    "        avg_sim = []\n",
    "        for cluster1 in group_sim[i]:\n",
    "            for cluster2 in group_sim[j]:\n",
    "                avg_sim.append(sim_mat[cluster1, cluster2])\n",
    "        between_group_sim[i,j] = between_group_sim[j,i] = np.array(avg_sim).mean()\n",
    "    \n",
    "    between_group_sim = np.abs(between_group_sim - 0.5) + 0.5\n",
    "    return within_group_sim, between_group_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-benjamin",
   "metadata": {},
   "source": [
    "## w/o NN imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-extraction",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clusters = collect_strainwise_clusters(feats, labels, embedding, thresh)\n",
    "tsim, tclusters = trim_clusters(clusters, similarity_matrix(sim), thresh=0.5)\n",
    "no_impute_glabs = embed_and_group_clusters(tsim)\n",
    "no_impute_group_frac = same_strain_grouping_frac(no_impute_glabs, tclusters)\n",
    "no_impute_wgs, no_impute_bgs = avg_group_sim(no_impute_glabs, tclusters, tsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-pleasure",
   "metadata": {},
   "source": [
    "## w/ NN imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = collect_strainwise_clusters(feats, labels, embedding, thresh)\n",
    "imputing_sim = impute_same_strain_values(sim, clusters)\n",
    "impute_tsim, impute_tclusters = trim_clusters(clusters, similarity_matrix(imputing_sim), thresh=0.5)\n",
    "impute_glabs = embed_and_group_clusters(impute_tsim)\n",
    "impute_group_frac = same_strain_grouping_frac(impute_glabs, impute_tclusters)\n",
    "impute_wgs, impute_bgs = avg_group_sim(impute_glabs, impute_tclusters, impute_tsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_frac = {\"strain\": [], \"type\": [], \"frac\": []}\n",
    "for strain in impute_group_frac.keys():\n",
    "    group_frac[\"strain\"].extend([strain, strain])\n",
    "    group_frac[\"type\"].append(\"impute\")\n",
    "    group_frac[\"frac\"].append(impute_group_frac[strain])\n",
    "    group_frac[\"type\"].append(\"no impute\")\n",
    "    group_frac[\"frac\"].append(no_impute_group_frac[strain])\n",
    "group_frac = pd.DataFrame.from_dict(group_frac)\n",
    "\n",
    "plt.figure(figsize=(5, 10))\n",
    "sns.barplot(x=\"frac\", y=\"strain\", hue=\"type\", data=group_frac)\n",
    "plt.plot([0.5, 0.5], plt.ylim(), \"--\", c=\"0.8\")\n",
    "plt.title(\"Strainwise Grouping Fraction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "axs[0][0].barh(list(no_impute_wgs.keys()), [x for _, x in no_impute_wgs.items()])\n",
    "axs[0][0].plot([0.5, 0.5], axs[0][0].get_ylim(), '--', color='0.8')\n",
    "axs[1][0].barh(list(impute_wgs.keys()), [x for _, x in impute_wgs.items()])\n",
    "axs[1][0].plot([0.5, 0.5], axs[1][0].get_ylim(), '--', color='0.8')\n",
    "sns.heatmap(no_impute_bgs, ax=axs[0][1])\n",
    "sns.heatmap(impute_bgs, ax=axs[1][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = similarity_matrix(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cv2.imshow(\"heatmap\", M)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "n = 22\n",
    "centers = np.vstack((np.zeros(2,), np.ones(2,)))\n",
    "X, y = make_blobs([2000, 500], n, centers=centers, cluster_std=1, shuffle=True)\n",
    "model = LinearDiscriminantAnalysis().fit(X,y)\n",
    "Xproj = model.transform(X)\n",
    "\n",
    "plt.scatter(Xproj, np.zeros_like(Xproj), c=y, s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped = umap.UMAP(n_components=2, n_neighbors=10, min_dist=0.0, metric=\"precomputed\").fit_transform(sim_mat)\n",
    "labels, _, glabels, _ = cluster_with_hdbscan(mapped, [0.5, 2.5], {\"prediction_data\": True, \"min_samples\": 1})\n",
    "idx, counts = np.unique(glabels, return_counts=True)\n",
    "plt.barh(idx, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mapped[:,0], mapped[:,1], s=0.5, c=glabels, cmap=\"Set1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-austria",
   "metadata": {},
   "source": [
    "# Analyze Strain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-lemon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:identified 42 clusters (max is 42) with min_sample_prop=0.4 and entropy_ratio=0.86\n",
      "INFO:root:identified 42 clusters (max is 42) with min_sample_prop=0.43 and entropy_ratio=0.86\n",
      "INFO:root:identified 41 clusters (max is 42) with min_sample_prop=0.47 and entropy_ratio=0.863\n",
      "INFO:root:identified 37 clusters (max is 42) with min_sample_prop=0.5 and entropy_ratio=0.864\n",
      "INFO:root:identified 6 clusters (max is 42) with min_sample_prop=0.53 and entropy_ratio=0.176\n",
      "INFO:root:identified 5 clusters (max is 42) with min_sample_prop=0.57 and entropy_ratio=0.175\n",
      "INFO:root:identified 4 clusters (max is 42) with min_sample_prop=0.6 and entropy_ratio=0.197\n",
      "INFO:root:identified 4 clusters (max is 42) with min_sample_prop=0.63 and entropy_ratio=0.197\n",
      "INFO:root:identified 4 clusters (max is 42) with min_sample_prop=0.67 and entropy_ratio=0.197\n",
      "INFO:root:identified 4 clusters (max is 42) with min_sample_prop=0.7 and entropy_ratio=0.197\n",
      "INFO:root:identified 4 clusters (max is 42) with min_sample_prop=0.73 and entropy_ratio=0.197\n",
      "INFO:root:identified 4 clusters (max is 42) with min_sample_prop=0.77 and entropy_ratio=0.197\n",
      "INFO:root:identified 4 clusters (max is 42) with min_sample_prop=0.8 and entropy_ratio=0.197\n",
      "INFO:root:identified 4 clusters (max is 42) with min_sample_prop=0.83 and entropy_ratio=0.197\n"
     ]
    }
   ],
   "source": [
    "strain = random.sample(list(labels.keys()), 1)[0]\n",
    "embed = umap.UMAP(n_components=2, n_neighbors=90, min_dist=0.0).fit_transform(StandardScaler().fit_transform(feats[strain]))\n",
    "\n",
    "strainwise_cluster_rng = [0.4, 1.2, 25]\n",
    "hdbscan_params = {\"prediction_data\": True, \"min_samples\": 1}\n",
    "labs, _, assgn, clusterer = cluster_with_hdbscan(embedding[strain], strainwise_cluster_rng, hdbscan_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-occasions",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(embed[:,0], embed[:,1], s=0.1, c=assgn, cmap=\"terrain\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-constitutional",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exem_idxs = clusterer.exemplars_indices_\n",
    "clusters = {i: feats[strain][exem_idxs[i]] for i in range(len(exem_idxs))}\n",
    "\n",
    "min_dist_sim, dbcv_index_sim, dens_sep_sim = [], [], []\n",
    "for i, j in combinations(range(len(exem_idxs)), 2):\n",
    "    min_dist_sim.append([minimum_distance_similarity(i, j, clusters, metric=\"cosine\"), i, j])\n",
    "    dbcv_index_sim.append([dbcv_index_similarity(i, j, clusters, metric=\"cosine\"), i, j])\n",
    "    dens_sep_sim.append([density_separation_similarity(i, j, clusters, metric=\"cosine\"), i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_max_min_sim(sim_data, exem_idxs, embed, assgn):\n",
    "    max_sim, i, j = max(sim_data, key=lambda x: x[0])\n",
    "    idxs = np.hstack([exem_idxs[i], exem_idxs[j]])\n",
    "    plt.scatter(embed[idxs,0], embed[idxs,1], s=0.1, color=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_idxs = np.hstack([exem_idxs[min_pair[0]], exem_idxs[min_pair[1]]])\n",
    "plt.scatter(embed[min_idxs,0], embed[min_idxs,1], s=0.1, c=assgn[min_idxs], cmap=\"Set1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idxs = np.hstack([exem_idxs[max_pair[0]], exem_idxs[max_pair[1]]])\n",
    "plt.scatter(embed[max_idxs,0], embed[max_idxs,1], s=0.1, c=assgn[max_idxs], cmap=\"Set1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "metric = \"cosine\"\n",
    "\n",
    "pair = max_pair\n",
    "print(cdist(feats[strain][exem_idxs[pair[0]]], feats[strain][exem_idxs[pair[1]]], metric=metric).min())\n",
    "\n",
    "pair = min_pair\n",
    "print(cdist(feats[strain][exem_idxs[pair[0]]], feats[strain][exem_idxs[pair[1]]], metric=metric).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-dealer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
